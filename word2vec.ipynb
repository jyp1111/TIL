{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "skip_gram.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyp1111/TIL/blob/master/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Iq8CRFpHWjy"
      },
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from os.path import isfile, join"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S6-BD6Wrkef"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92AF5z03rkef"
      },
      "source": [
        "# 전처리 과정 코드에 다 쓰고 싶었는데 학원 컴퓨터에 있는 듯 합니다.\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "with open(\"wiki_00\",mode=\"r\",encoding=\"utf-8\") as f:\n",
        "    text=f.read()\n",
        "textdf=pd.DataFrame(text.split(\"</doc>\")[:-1])\n",
        "textdf.columns=[\"content\"]\n",
        "textdf[\"content\"]=textdf[\"content\"].str.replace('<.*?>',\"\").str.replace(\"\\n\",\" \")\n",
        "textdf[\"tokenize\"]=textdf[\"content\"].apply(word_tokenize)\n",
        "textdf[\"pos_tag\"]=textdf[\"tokenize\"].apply(pos_tag)\n",
        "\n",
        "pos=[\"JJ\",\"JJR\", \"JJS\",\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]\n",
        "\n",
        "textdf[\"reduced_postag\"]=textdf[\"pos_tag\"].apply(lambda ls: [v for v in ls if (v[1] in pos) and (len(v[0])>1)])\n",
        "textdf[\"token_ls\"]=textdf[\"reduced_postag\"].apply(lambda ls: [v[0] for v in ls])\n",
        "\n",
        "df=pd.DataFrame([\",\".join(token_ls) for token_ls in textdf[\"token_ls\"].tolist()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4rr1oyVrkeh"
      },
      "source": [
        "class construction:\n",
        "    def __init__(self,docs_ls_file,word_freq_file):\n",
        "\n",
        "        if not isfile(join(\"./\",docs_ls_file)):\n",
        "            self.docs_ls_scv=pd.Series(dtype=\"object\")\n",
        "            self.word_freq=Counter()\n",
        "            \n",
        "        else:\n",
        "            self.docs_ls_csv=pd.read_csv(docs_ls_file)[\"0\"]    \n",
        "            \n",
        "            with open(word_freq_file,\"rb\") as f:\n",
        "                self.word_freq=pickle.load(f)\n",
        "        \n",
        "        self.freq_ls=None\n",
        "        self.index_word_dic=None\n",
        "        self.word_index_dic=None\n",
        "        \n",
        "    def add_docs_ls(self,docs_ls,docs_ls_file,word_freq_file):\n",
        "        concate_docs_ls=pd.concat([self.docs_ls_csv,docs_ls])\n",
        "        self.word_freq+=Counter(sum(docs_ls.tolist(),[]))\n",
        "        \n",
        "        concate_docs_ls.to_csv(docs_ls_file,index=False)\n",
        "        \n",
        "        self.docs_ls_csv=pd.read_csv(docs_ls_file)[\"0\"]\n",
        "        with open(word_freq_file,'wb') as f:\n",
        "            pickle.dump(self.word_freq, f)\n",
        "\n",
        "    def get_word_freq_ls(self,vocab_size):\n",
        "        word_ls,self.freq_ls=zip(*self.word_freq.most_common()[:vocab_size])\n",
        "        self.index_word_dic=dict(enumerate(word_ls))\n",
        "        self.word_index_dic={}\n",
        "        for i,w in enumerate(word_ls):\n",
        "            self.word_index_dic[w]=i\n",
        "        with open(\"freq_ls.pickle\",\"wb\") as f:\n",
        "            pickle.dump(self.freq_ls,f)\n",
        "        with open(\"index_word_dic.pickle\",\"wb\") as f:\n",
        "            pickle.dump(self.index_word_dic,f)\n",
        "        with open(\"word_index_dic.pickle\",\"wb\") as f:\n",
        "            pickle.dump(self.word_index_dic,f)\n",
        "            \n",
        "    def set_docs_ls(self):\n",
        "        # 문서수 : 6041696\n",
        "        for i in range(-(-6041696//10000)):\n",
        "            with open(f\"docs_ls/docs_ls{i}\",\"wb\") as f:\n",
        "                pickle.dump(self.docs_ls_csv[i*10000:(i+1)*10000].str.replace(\"[\",\"\").str.replace(\"]\",\"\").str.replace(\"'\",\"\").str.replace(\" \",\"\").str.split(\",\"), f)\n",
        "\n",
        "class skip_gram:\n",
        "    def __init__(self,docs_ls_file,window_size):\n",
        "        with open(\"freq_ls.pickle\",\"rb\") as f:\n",
        "            self.freq_ls=pickle.load(f)\n",
        "        with open(\"index_word_dic.pickle\",\"rb\") as f:\n",
        "            self.index_word_dic=pickle.load(f)\n",
        "        with open(\"word_index_dic.pickle\",\"rb\") as f:\n",
        "            self.word_index_dic=pickle.load(f)\n",
        "        with open(docs_ls_file,\"rb\") as f:\n",
        "            self.docs_ls=pickle.load(f)\n",
        "\n",
        "        self.window_size=window_size\n",
        "        if not isfile(join(\"./\",\"coef.pickle\")):\n",
        "            self.coef_=None\n",
        "        else:  \n",
        "            with open(\"coef.pickle\",\"rb\") as f:\n",
        "                self.coef_=pickle.load(f)        \n",
        "\n",
        "    def sigmoid(self,x):\n",
        "        return 1/(1+np.exp(-x))\n",
        "    \n",
        "    def negative_sampling(self):\n",
        "        n=len(self.word_index_dic)\n",
        "        fre_ls=np.array(list(self.freq_ls))\n",
        "        distribution=np.power(fre_ls,0.75)/np.power(fre_ls,0.75).sum()\n",
        "        return np.random.choice(range(n), size=20, p=distribution)\n",
        "\n",
        "    \n",
        "    def connection(self,doc,i):\n",
        "        result=set()\n",
        "        for j,v in enumerate(doc):\n",
        "            if v==self.index_word_dic[i]:\n",
        "                result=result.union({self.word_index_dic[doc[k]] for k in range(max(j-self.window_size,0),min(j+self.window_size+1,len(doc))) if doc[k] in self.word_index_dic})\n",
        "        return result-{i}            \n",
        "\n",
        "    def train(self,vec_dim,learning_rate):\n",
        "        n=len(self.word_index_dic)\n",
        "        m=len(self.docs_ls)\n",
        "        \n",
        "        if self.coef_==None:\n",
        "            W_1=np.random.rand(n,vec_dim)\n",
        "            W_2=np.random.rand(n,vec_dim)\n",
        "        else: \n",
        "            W_1,W_2=self.coef_\n",
        "\n",
        "        for i in range(n):\n",
        "            return_set=set().union(*self.docs_ls.apply(self.connection,args=(i,)))\n",
        "\n",
        "            neighbor=sorted(list(return_set))\n",
        "            negative_words=self.negative_sampling()\n",
        "\n",
        "            positive=np.ones(len(neighbor))\n",
        "            negative=np.zeros(20)\n",
        "\n",
        "            y=np.concatenate((positive,negative))\n",
        "            relative_indices=neighbor+list(negative_words)\n",
        "            hyp=self.sigmoid(np.dot(W_2[relative_indices],W_1[i]))\n",
        "            loss=-(y*np.log(hyp)+(1-y)*np.log(1-hyp)).mean()\n",
        "\n",
        "            first_par=W_1[i]\n",
        "            W_1[i]-=np.dot(W_2[relative_indices].T,hyp-y)*learning_rate/len(y)\n",
        "            W_2[relative_indices]-=np.dot((hyp-y).reshape(-1,1),first_par.reshape(1,-1))*learning_rate/len(y)\n",
        "            if i%100==0:\n",
        "                print(i)\n",
        "        self.coef_=(W_1,W_2)\n",
        "        with open(\"coef.pickle\",\"wb\") as f:\n",
        "            pickle.dump(self.coef_,f)\n",
        "        \n",
        "    def check(self,word1,word2,word3):\n",
        "        word_matrix=self.coef_[0]\n",
        "        \n",
        "        word1_index=self.word_index_dic[word1]\n",
        "        word2_index=self.word_index_dic[word2]\n",
        "        word3_index=self.word_index_dic[word3]\n",
        "        result_vec=word_matrix[word1_index]-word_matrix[word2_index]+word_matrix[word3_index]\n",
        "        \n",
        "        return self.index_word_dic[np.argmin(np.multiply(word_matrix-result_vec,word_matrix-result_vec).sum(axis=1))]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAo-0e5Xrkej"
      },
      "source": [
        "Construction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TamAzmI4rkej"
      },
      "source": [
        "import csv\n",
        "from os import listdir\n",
        "from os.path import isfile, join, isdir\n",
        "csv_files=[\"./text_csv/\"+f  for f in listdir(\"./text_csv/\")  if isfile(join(\"./text_csv/\", f))]\n",
        "csv_files.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTmlx1tqrkel",
        "outputId": "d6bbe23b-211e-46f7-c685-330ec5b77e78"
      },
      "source": [
        "len(csv_files)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "139"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvujB90Drkeo"
      },
      "source": [
        "for x in csv_files:\n",
        "    df=pd.read_csv(x)\n",
        "    df=df.dropna()\n",
        "    df[\"0\"]=df[\"0\"].apply(lambda v : \",\".join(v.split(\",\")[1:]))\n",
        "    df=df.dropna()\n",
        "    df[\"0\"].to_csv(x,index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4nmHmSJrkep",
        "outputId": "ccd8eb68-7e27-4c3e-fd09-a8353a700e11"
      },
      "source": [
        "wiki=construction(\"docs_ls.csv\",\"freq.pickle\")\n",
        "for file in csv_files[135:]:\n",
        "    df=pd.read_csv(file)\n",
        "    wiki.add_docs_ls(df[\"0\"].str.split(\",\"),\"docs_ls.csv\",\"freq.pickle\")\n",
        "    print(file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./text_csv/FF.csv\n",
            "./text_csv/FG.csv\n",
            "./text_csv/FH.csv\n",
            "./text_csv/FI.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQWqD4m2rkes"
      },
      "source": [
        "wiki=construction(\"docs_ls.csv\",\"freq.pickle\")\n",
        "wiki.get_word_freq_ls(30000)\n",
        "wiki.set_docs_ls()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD_O2QY4rket"
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join, isdir\n",
        "docs_ls_files=[\"./docs_ls/\"+f  for f in listdir(\"./docs_ls/\")  if isfile(join(\"./docs_ls/\", f))]\n",
        "docs_ls_files.sort(key=lambda x:int(x[17:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv8sTIvyrkev",
        "outputId": "80d98710-c419-4d85-e98b-011ee7e58167"
      },
      "source": [
        "len(docs_ls_files)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "605"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryAwl_kOrkex"
      },
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViLCOodNrkex",
        "outputId": "3a52b958-3b11-4511-f236-16e36df682a9"
      },
      "source": [
        "model=skip_gram(docs_ls_files[0],1)\n",
        "model.train(3,0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n",
            "4700\n",
            "4800\n",
            "4900\n",
            "5000\n",
            "5100\n",
            "5200\n",
            "5300\n",
            "5400\n",
            "5500\n",
            "5600\n",
            "5700\n",
            "5800\n",
            "5900\n",
            "6000\n",
            "6100\n",
            "6200\n",
            "6300\n",
            "6400\n",
            "6500\n",
            "6600\n",
            "6700\n",
            "6800\n",
            "6900\n",
            "7000\n",
            "7100\n",
            "7200\n",
            "7300\n",
            "7400\n",
            "7500\n",
            "7600\n",
            "7700\n",
            "7800\n",
            "7900\n",
            "8000\n",
            "8100\n",
            "8200\n",
            "8300\n",
            "8400\n",
            "8500\n",
            "8600\n",
            "8700\n",
            "8800\n",
            "8900\n",
            "9000\n",
            "9100\n",
            "9200\n",
            "9300\n",
            "9400\n",
            "9500\n",
            "9600\n",
            "9700\n",
            "9800\n",
            "9900\n",
            "10000\n",
            "10100\n",
            "10200\n",
            "10300\n",
            "10400\n",
            "10500\n",
            "10600\n",
            "10700\n",
            "10800\n",
            "10900\n",
            "11000\n",
            "11100\n",
            "11200\n",
            "11300\n",
            "11400\n",
            "11500\n",
            "11600\n",
            "11700\n",
            "11800\n",
            "11900\n",
            "12000\n",
            "12100\n",
            "12200\n",
            "12300\n",
            "12400\n",
            "12500\n",
            "12600\n",
            "12700\n",
            "12800\n",
            "12900\n",
            "13000\n",
            "13100\n",
            "13200\n",
            "13300\n",
            "13400\n",
            "13500\n",
            "13600\n",
            "13700\n",
            "13800\n",
            "13900\n",
            "14000\n",
            "14100\n",
            "14200\n",
            "14300\n",
            "14400\n",
            "14500\n",
            "14600\n",
            "14700\n",
            "14800\n",
            "14900\n",
            "15000\n",
            "15100\n",
            "15200\n",
            "15300\n",
            "15400\n",
            "15500\n",
            "15600\n",
            "15700\n",
            "15800\n",
            "15900\n",
            "16000\n",
            "16100\n",
            "16200\n",
            "16300\n",
            "16400\n",
            "16500\n",
            "16600\n",
            "16700\n",
            "16800\n",
            "16900\n",
            "17000\n",
            "17100\n",
            "17200\n",
            "17300\n",
            "17400\n",
            "17500\n",
            "17600\n",
            "17700\n",
            "17800\n",
            "17900\n",
            "18000\n",
            "18100\n",
            "18200\n",
            "18300\n",
            "18400\n",
            "18500\n",
            "18600\n",
            "18700\n",
            "18800\n",
            "18900\n",
            "19000\n",
            "19100\n",
            "19200\n",
            "19300\n",
            "19400\n",
            "19500\n",
            "19600\n",
            "19700\n",
            "19800\n",
            "19900\n",
            "20000\n",
            "20100\n",
            "20200\n",
            "20300\n",
            "20400\n",
            "20500\n",
            "20600\n",
            "20700\n",
            "20800\n",
            "20900\n",
            "21000\n",
            "21100\n",
            "21200\n",
            "21300\n",
            "21400\n",
            "21500\n",
            "21600\n",
            "21700\n",
            "21800\n",
            "21900\n",
            "22000\n",
            "22100\n",
            "22200\n",
            "22300\n",
            "22400\n",
            "22500\n",
            "22600\n",
            "22700\n",
            "22800\n",
            "22900\n",
            "23000\n",
            "23100\n",
            "23200\n",
            "23300\n",
            "23400\n",
            "23500\n",
            "23600\n",
            "23700\n",
            "23800\n",
            "23900\n",
            "24000\n",
            "24100\n",
            "24200\n",
            "24300\n",
            "24400\n",
            "24500\n",
            "24600\n",
            "24700\n",
            "24800\n",
            "24900\n",
            "25000\n",
            "25100\n",
            "25200\n",
            "25300\n",
            "25400\n",
            "25500\n",
            "25600\n",
            "25700\n",
            "25800\n",
            "25900\n",
            "26000\n",
            "26100\n",
            "26200\n",
            "26300\n",
            "26400\n",
            "26500\n",
            "26600\n",
            "26700\n",
            "26800\n",
            "26900\n",
            "27000\n",
            "27100\n",
            "27200\n",
            "27300\n",
            "27400\n",
            "27500\n",
            "27600\n",
            "27700\n",
            "27800\n",
            "27900\n",
            "28000\n",
            "28100\n",
            "28200\n",
            "28300\n",
            "28400\n",
            "28500\n",
            "28600\n",
            "28700\n",
            "28800\n",
            "28900\n",
            "29000\n",
            "29100\n",
            "29200\n",
            "29300\n",
            "29400\n",
            "29500\n",
            "29600\n",
            "29700\n",
            "29800\n",
            "29900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH7ZDF_Nrke0",
        "outputId": "9d1a1eb4-2d1c-475d-9c27-db49c95c3e2a"
      },
      "source": [
        "model.check(\"Athens\",\"Greece\",\"Korea\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'thwarted'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkJuO8F_rke1"
      },
      "source": [
        "for docs_ls_file in docs_ls_files:\n",
        "    model=skip_gram(docs_ls_file,1)\n",
        "    model.train(3,0.01)\n",
        "model.check(\"Athens\",\"Greece\",\"Korea\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}